{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    R = 2*len(shared_words_in_q1)/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "eps = 5000 \n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:17: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "q_dict = defaultdict(set)\n",
    "ques = pd.concat([df_train[['question1', 'question2']],df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "def q1_freq(row):\n",
    "    return(len(q_dict[row['question1']]))\n",
    "    \n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row['question2']]))\n",
    "    \n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:17: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.DataFrame()\n",
    "x_test = pd.DataFrame()\n",
    "x_train['word_match'] = df_train.apply(word_match_share, axis=1, raw=True)\n",
    "x_train['tfidf_word_match'] = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "x_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "x_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n",
    "x_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n",
    "x_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)\n",
    "x_test['tfidf_word_match'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "x_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "x_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n",
    "x_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n",
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]\n",
    "p = 0.165\n",
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "\n",
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.679414\tvalid-logloss:0.679473\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.567426\tvalid-logloss:0.567989\n",
      "[20]\ttrain-logloss:0.48825\tvalid-logloss:0.489194\n",
      "[30]\ttrain-logloss:0.430304\tvalid-logloss:0.431549\n",
      "[40]\ttrain-logloss:0.386895\tvalid-logloss:0.388407\n",
      "[50]\ttrain-logloss:0.35366\tvalid-logloss:0.355421\n",
      "[60]\ttrain-logloss:0.327965\tvalid-logloss:0.329964\n",
      "[70]\ttrain-logloss:0.30792\tvalid-logloss:0.310102\n",
      "[80]\ttrain-logloss:0.292252\tvalid-logloss:0.294604\n",
      "[90]\ttrain-logloss:0.279946\tvalid-logloss:0.282441\n",
      "[100]\ttrain-logloss:0.270292\tvalid-logloss:0.272905\n",
      "[110]\ttrain-logloss:0.262489\tvalid-logloss:0.265209\n",
      "[120]\ttrain-logloss:0.255829\tvalid-logloss:0.25864\n",
      "[130]\ttrain-logloss:0.250483\tvalid-logloss:0.253376\n",
      "[140]\ttrain-logloss:0.245983\tvalid-logloss:0.248943\n",
      "[150]\ttrain-logloss:0.242276\tvalid-logloss:0.245282\n",
      "[160]\ttrain-logloss:0.23922\tvalid-logloss:0.242264\n",
      "[170]\ttrain-logloss:0.23659\tvalid-logloss:0.239665\n",
      "[180]\ttrain-logloss:0.234509\tvalid-logloss:0.237628\n",
      "[190]\ttrain-logloss:0.232779\tvalid-logloss:0.235934\n",
      "[200]\ttrain-logloss:0.23135\tvalid-logloss:0.234529\n",
      "[210]\ttrain-logloss:0.230114\tvalid-logloss:0.233316\n",
      "[220]\ttrain-logloss:0.229128\tvalid-logloss:0.232357\n",
      "[230]\ttrain-logloss:0.228266\tvalid-logloss:0.231525\n",
      "[240]\ttrain-logloss:0.227406\tvalid-logloss:0.230691\n",
      "[250]\ttrain-logloss:0.226705\tvalid-logloss:0.230011\n",
      "[260]\ttrain-logloss:0.226072\tvalid-logloss:0.229408\n",
      "[270]\ttrain-logloss:0.225521\tvalid-logloss:0.228878\n",
      "[280]\ttrain-logloss:0.225026\tvalid-logloss:0.228411\n",
      "[290]\ttrain-logloss:0.224569\tvalid-logloss:0.227979\n",
      "[300]\ttrain-logloss:0.224133\tvalid-logloss:0.22757\n",
      "[310]\ttrain-logloss:0.223702\tvalid-logloss:0.227172\n",
      "[320]\ttrain-logloss:0.223315\tvalid-logloss:0.226809\n",
      "[330]\ttrain-logloss:0.222894\tvalid-logloss:0.226404\n",
      "[340]\ttrain-logloss:0.222514\tvalid-logloss:0.226039\n",
      "[350]\ttrain-logloss:0.222189\tvalid-logloss:0.225727\n",
      "[360]\ttrain-logloss:0.221885\tvalid-logloss:0.225435\n",
      "[370]\ttrain-logloss:0.221606\tvalid-logloss:0.225162\n",
      "[380]\ttrain-logloss:0.221363\tvalid-logloss:0.224936\n",
      "[390]\ttrain-logloss:0.221113\tvalid-logloss:0.224702\n",
      "[399]\ttrain-logloss:0.220892\tvalid-logloss:0.224491\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_test = xgb.DMatrix(x_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv('simple_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
